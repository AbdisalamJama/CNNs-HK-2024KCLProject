{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D-CNNs for MNIST dataset\n",
    "* Learn about:\n",
    "    * [THE MNIST DATABASE](http://yann.lecun.com/exdb/mnist/)\n",
    "    * Coding with PyTorch Lightning\n",
    "        * LightningDataModule\n",
    "        * LightningModule\n",
    "        * (optional) Logger, Trainer, callbacks\n",
    "    * Two types of the artificial neuralnetworks\n",
    "        * Multilayer perceptron (MLP)\n",
    "        * 2D Convolutional neuralnetworks (2D CNNs)\n",
    "* Let's try:\n",
    "    * Build MLP or CNNs by yourself\n",
    "    * Try to use other loss functions\n",
    "    * Try to change the training parameters\n",
    "        * learning rate, batch size, epochs, etc..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1. Lightning data module for MNIST dataset ##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `train_data`: 50,000\n",
    "* `test_data`: 10,000\n",
    "* `val_data`: 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from typing import Optional\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HKDataset(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 128,\n",
    "        num_workers: int = 2,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.mean = ()\n",
    "        self.std = ()\n",
    "        self.trans = None\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        imgs = torch.load('../hyperk_img/imgs_scaled.pt')\n",
    "        labels_targets = torch.load('../hyperk_img/labels_targets_scaled.pt')\n",
    "        indices = torch.load('../hyperk_img/indices.pt')\n",
    "\n",
    "        self.train_data = torch.utils.data.TensorDataset( imgs[indices['train']], labels_targets[indices['train']])\n",
    "        self.valid_data = torch.utils.data.TensorDataset( imgs[indices['valid']], labels_targets[indices['valid']])\n",
    "        self.test_data = torch.utils.data.TensorDataset( imgs[indices['test']], labels_targets[indices['test']])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_data, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2-2. 2D-CNNs model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class CNNs2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNs2D, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2, out_channels=32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.fc_class_layer = nn.Sequential(\n",
    "            nn.Linear(1024,2), # 2class\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.fc_regression_layer = nn.Sequential(\n",
    "            nn.Linear(1024, 4), # 4 regression targets\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        #print('x0', x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print('x4', x.shape, x)\n",
    "        return [self.fc_class_layer(x), self.fc_regression_layer(x)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3. Pytorch lightning module ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "\n",
    "class PLModuleMNIST(LightningModule):\n",
    "    def __init__(self, model, lr_rate):\n",
    "        super(PLModuleMNIST, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr_rate = lr_rate\n",
    "        self.accuracy_calc = BinaryAccuracy(threshold=0.5)\n",
    "        \n",
    "        self.automatic_optimization = False\n",
    "        self.loss_training = []\n",
    "        self.loss_valid = []\n",
    "        self.loss_training_epoch = []\n",
    "        self.loss_valid_epoch = []\n",
    "        \n",
    "        self.acc_training = []\n",
    "        self.acc_valid = []\n",
    "        self.acc_training_epoch = []\n",
    "        self.acc_valid_epoch = []\n",
    "\n",
    "    def forward(self, batch: Tensor, **kwargs) -> Tensor:\n",
    "        #print('batch:', batch.size)\n",
    "        return self.model(batch)\n",
    "\n",
    "    #def loss_fn(self, x, y):\n",
    "    #    #print('loss:x,y',x.shape, y.shape)\n",
    "    #    #print('type',x[0].dtype, y[0].dtype)\n",
    "    #    return F.nll_loss(x, y)\n",
    "    \n",
    "    #def loss_fn_regression(self, x, y):\n",
    "    #    #print(x[0][0], y[0][0])\n",
    "    #    loss_energy = F.l1_loss(x[:,0], y[:,0])\n",
    "    #    loss_position = F.l1_loss(x[:,1:4], y[:,1:4])\n",
    "    #    loss = loss_energy + loss_position\n",
    "    #    return loss\n",
    "    \n",
    "    def loss_fn(self, x, y):\n",
    "        x_class=x[0]\n",
    "        y_class=torch.tensor(y[:,0], dtype=torch.int64)\n",
    "        x_reg=x[1]\n",
    "        y_reg=y[:,1:]\n",
    "        \n",
    "        loss_class = F.nll_loss(x_class, y_class)\n",
    "        loss_energy = F.l1_loss(x_reg[:,0], y_reg[:,0])\n",
    "        loss_position = F.l1_loss(x_reg[:,1:4], y_reg[:,1:4])\n",
    "        loss = loss_class + loss_energy + loss_position\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        opt = self.optimizers()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        x, y = batch\n",
    "        label=torch.tensor(y[:,0], dtype=torch.int64)\n",
    "        #targets=y[:,1:5]\n",
    "        \n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss_fn(logits,y)\n",
    "        #loss = self.loss_fn(logits,y[:,0])\n",
    "        #loss = self.loss_fn_regression(logits,targets)\n",
    "        \n",
    "        acc = self.accuracy_calc(logits[0][:,1], label)\n",
    "        \n",
    "        self.manual_backward(loss)\n",
    "        opt.step()\n",
    "\n",
    "        self.loss_training.append(loss)\n",
    "        self.acc_training.append(acc)        \n",
    "\n",
    "        \n",
    "    def validation_step(self, batch):\n",
    "        x, y = batch\n",
    "        \n",
    "        label=torch.tensor(y[:,0], dtype=torch.int64)\n",
    "        #targets=y[:,1:5]\n",
    "        #print('val:x,y', x.shape, y.shape)\n",
    "        #print('type',type(x), type(y))\n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss_fn(logits,y)\n",
    "        acc = self.accuracy_calc(logits[0][:,1], label)\n",
    "        #loss = self.loss_fn(logits,y[:,0])\n",
    "        #loss = self.loss_fn_regression(logits,targets)\n",
    "        #acc = self.accuracy_calc(logits, y[:,0])\n",
    "        self.loss_valid.append(loss)\n",
    "        self.acc_valid.append(acc)\n",
    "\n",
    "    \n",
    "    def test_step(self, batch):\n",
    "        x, y = batch\n",
    "        label=torch.tensor(y[:,0], dtype=torch.int64)\n",
    "        #targets=y[:,1:5]\n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss_fn(logits,y)\n",
    "        acc = self.accuracy_calc(x[1], label)\n",
    "        \n",
    "        #loss = self.loss_fn(logits,y[:,0])\n",
    "        #loss = self.loss_fn_regression(logits,targets)\n",
    "        #acc = self.accuracy_calc(logits, y[:,0])\n",
    "\n",
    "    \n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        avg_loss = torch.stack(self.loss_training).mean()\n",
    "        avg_acc = torch.stack(self.acc_training).mean()\n",
    "        self.loss_training.clear()\n",
    "        self.acc_training.clear()\n",
    "        #print('train_loss:', avg_loss, 'train_acc', avg_acc)\n",
    "        self.loss_training_epoch.append(avg_loss)\n",
    "        self.acc_training_epoch.append(avg_acc)\n",
    "        self.log('loss/train',avg_loss)\n",
    "        self.log('acc/train',avg_acc)\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        avg_loss = torch.stack(self.loss_valid).mean()\n",
    "        avg_acc = torch.stack(self.acc_valid).mean()\n",
    "        self.loss_valid.clear()\n",
    "        self.acc_valid.clear()\n",
    "        self.loss_valid_epoch.append(avg_loss)\n",
    "        self.acc_valid_epoch.append(avg_acc)\n",
    "        self.log('loss/valid',avg_loss)\n",
    "        self.log('acc/valid',avg_acc)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer =  optim.Adam(self.parameters(), lr=self.lr_rate)\n",
    "        lr_scheduler = {'scheduler': optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9), 'name': 'exp_LR'}\n",
    "        return [optimizer], [lr_scheduler]\n",
    "        #return optim.Adam(self.model.parameters(), lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4. Logger ##\n",
    "Tensorboard logger is used. A window of tensorboard opens here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(save_dir='./tb_logs/', name = 'HK')\n",
    "\n",
    "\n",
    "#%reload_ext tensorboard\n",
    "#%tensorboard --logdir=./tb_logs/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5. Build & fit a model ##\n",
    "Select whether to use `MLP()` or `CNNs2D()` for the model. The settings for the learning is described here.\n",
    "* `lr_rate`: learning rate\n",
    "* `batch_size`\n",
    "* `max_epochs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_core = CNNs2D() # or MLP()\n",
    "#model_core = MLP()\n",
    "model = PLModuleMNIST(model = model_core, lr_rate=0.001)\n",
    "dm = HKDataset(batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/tmp/ipykernel_72489/3446991305.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  imgs = torch.load('../hyperk_img/imgs_scaled.pt')\n",
      "/tmp/ipykernel_72489/3446991305.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels_targets = torch.load('../hyperk_img/labels_targets_scaled.pt')\n",
      "/tmp/ipykernel_72489/3446991305.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  indices = torch.load('../hyperk_img/indices.pt')\n",
      "\n",
      "  | Name          | Type           | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model         | CNNs2D         | 39.7 K | train\n",
      "1 | accuracy_calc | BinaryAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "39.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "39.7 K    Total params\n",
      "0.159     Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72489/1433470696.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label=torch.tensor(y[:,0], dtype=torch.int64)\n",
      "/tmp/ipykernel_72489/1433470696.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_class=torch.tensor(y[:,0], dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/79 [00:00<?, ?it/s]                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72489/1433470696.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label=torch.tensor(y[:,0], dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/79 [00:00<?, ?it/s, v_num=6]         "
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "\n",
    "runner = Trainer(max_epochs=10, \n",
    "                 #gpus=[0],\n",
    "                 accelerator='cpu',  # or 'gpu', 'auto'\n",
    "                 logger=logger, \n",
    "                 callbacks=[TQDMProgressBar(refresh_rate=10)])\n",
    "\n",
    "\n",
    "runner.fit(model, dm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6. Check the trained model ##\n",
    "* Structure of the model\n",
    "* Classification performance for the test dataset\n",
    "* Feature maps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step6-1. Structure of the trained model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can load the trained model at the specific checkpoint\n",
    "#model = PLModuleMNIST.load_from_checkpoint(model=model_core, lr_rate=0.001, checkpoint_path='./tb_logs/MNIST/version_0/checkpoints/epoch=2-step=1173.ckpt')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_targets = []\n",
    "#outputs = []\n",
    "outputs_class = []\n",
    "outputs_targets = []\n",
    "for batch in dm.test_dataloader():\n",
    "    x, y = batch \n",
    "    \n",
    "    outputs = model.model(x)\n",
    "    out_class = outputs[0].to('cpu').detach()\n",
    "    out_targets =  outputs[1].to('cpu').detach()\n",
    "    outputs_class.append(out_class)\n",
    "    outputs_targets.append(out_targets)\n",
    "    labels_targets.append(y.to('cpu').detach())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_targets=np.vstack(labels_targets)\n",
    "outputs_class=np.vstack(outputs_class)\n",
    "outputs_targets=np.vstack(outputs_targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.save(labels_targets, 'test_labels_targets.pt')\n",
    "torch.save(outputs_class, 'test_outputs_class.pt')\n",
    "torch.save(outputs_targets, 'test_outputs_targets.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step6-2. Classification perfromance for the test dataset ####\n",
    "`Loss` and `Accuracy` are useful metrics for understanding the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss_train = [i.to('cpu').detach().numpy() for i in model.loss_training_epoch]\n",
    "loss_valid = [i.to('cpu').detach().numpy() for i in model.loss_valid_epoch]\n",
    "\n",
    "plt.plot(loss_train, label='train')\n",
    "plt.plot(loss_valid, label='valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#plt.ylim([None, 1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "acc_train = [i.to('cpu').detach().numpy() for i in model.acc_training_epoch]\n",
    "acc_valid = [i.to('cpu').detach().numpy() for i in model.acc_valid_epoch]\n",
    "\n",
    "plt.plot(acc_train, label='train')\n",
    "plt.plot(acc_valid, label='valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#plt.ylim([None, 1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
